from difflib import SequenceMatcher
from typing_extensions import Any

import test

from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge 
from nltk.translate.meteor_score import meteor_score
from bert_score import score
from sentence_transformers import SentenceTransformer, util

def calculate_sql_similarity(
    generated_sql: str, 
    expected_sql:  str
) -> float:
    """
    Calculate similarity between the generated SQL and expected SQL.
    
    Parameters:
    - generated_sql (str): The SQL query generated by the model.
    - expected_sql (str): The reference SQL query in the dataset.
    
    Returns:
    - float: Similarity score between 0 and 1, where 1 means an exact match.
    """

    return SequenceMatcher(None, generated_sql, expected_sql).ratio()


def determine_accuracy(
    generated_and_expected_queries: list[tuple[str, str]], 
    threshold:                      float = 0.9
) -> float:
    """
    Determine accuracy based on SQL similarity for multiple queries.
    
    Parameters:
    - generated_sql_list (list[str]): list of generated SQL queries.
    - expected_sql_list (list[str]): list of expected SQL queries from the dataset.
    - threshold (float): Minimum similarity ratio to count as correct.
    
    Returns:
    - float: Accuracy as the proportion of correct SQL queries.
    """

    if not generated_and_expected_queries:
        test.logging.warning("Empty results array")
        return -1.0
    
    correct_count: int = 0
    for generated_query, expected_query in generated_and_expected_queries:
        similarity: float = calculate_sql_similarity(generated_query, expected_query)
        if similarity >= threshold:
            correct_count += 1
    
    accuracy: float = correct_count / len(generated_and_expected_queries)
    return accuracy

def BLEU_metric(generated_and_expected_queries: list[tuple[str, str]]) -> float:
    bleu_scores: list[Any] = []

    for generated_query, expected_query in generated_and_expected_queries:
        score = sentence_bleu([expected_query.split()], generated_query.split())
        bleu_scores.append(score)

    return sum(bleu_scores) / len(bleu_scores)

def ROUGE_metric(generated_and_expected_queries: list[tuple[str, str]]) -> Any:
    rouge: Rouge = Rouge()
    generated_queries, expected_queries = zip(*generated_and_expected_queries)

    scores = rouge.get_scores(generated_queries, expected_queries, avg=True)
    return scores

def METEOR_metric(generated_and_expected_queries: list[tuple[str, str]]) -> float:
    meteor_scores = []

    for generated_query, expected_query in generated_and_expected_queries:
        score = meteor_score([generated_query], expected_query)
        meteor_scores.append(score)

    return sum(meteor_scores) / len(meteor_scores)

def BERTScore_metric(generated_and_expected_queries: list[tuple[str, str]]) -> tuple[Any, Any, Any]:
    generated_queries, expected_queries = zip(*generated_and_expected_queries)

    P, R, F1 = score(generated_queries, expected_queries, lang="en", verbose=False)
    return (P, R, F1)


def SemScore_metric(generated_and_expected_queries: list[tuple[str, str]]) -> float:
    model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2')
    generated_queries, expected_queries = zip(*generated_and_expected_queries)

    generated_embeddings = model.encode(generated_queries, convert_to_tensor=True)
    expected_embeddings = model.encode(expected_queries, convert_to_tensor=True)

    similarity_scores = [
        util.cos_sim(generated_embedding, expected_embedding).item() 
        for generated_embedding, expected_embedding 
        in zip(generated_embeddings, expected_embeddings)
    ]

    return sum(similarity_scores) / len(similarity_scores)
